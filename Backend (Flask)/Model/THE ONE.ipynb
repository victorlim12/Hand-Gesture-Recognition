{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1af62cd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmediapipe\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mmp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mthreading\u001b[39;00m \u001b[39mimport\u001b[39;00m Thread\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodel\u001b[39;00m \u001b[39mimport\u001b[39;00m TorchVisionModel\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "from threading import Thread\n",
    "\n",
    "from model import TorchVisionModel\n",
    "from ssd_mobilenetv3 import SSDMobilenet\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image, ImageOps\n",
    "from torch import Tensor\n",
    "from torchvision.transforms import functional as f\n",
    "import os\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Basic mp_pose definition\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Basic mp_hands definition\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "\n",
    "# Global Variables\n",
    "flag_wait = 0\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "hands = mp_hands.Hands(min_detection_confidence=0.8, min_tracking_confidence=0.5)\n",
    "\n",
    "# Pose Detection Function\n",
    "\n",
    "def Pose_detection(result,index):\n",
    "    # Detect\n",
    "    results = pose.process(image)\n",
    "    #print(dir(mp_pose.PoseLandmark))\n",
    "    #if(results.pose_landmarks):\n",
    "        #print(results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_ELBOW])\n",
    "    \n",
    "    #Return draw params\n",
    "    result[index] = [results.pose_landmarks,mp_pose.POSE_CONNECTIONS,mp_drawing_styles.get_default_pose_landmarks_style()]\n",
    "    \n",
    "    #Wait for race (Doesn't wait)\n",
    "    #if(flag_wait == 2):\n",
    "    #    flag_wait = 0\n",
    "    #flag_wait += 1\n",
    "\n",
    "    \n",
    "# Hands Detection function     \n",
    "def Hands_detection(result,index):\n",
    "    #Detect\n",
    "    results = hands.process(image)\n",
    "\n",
    "    #if(results.multi_hand_landmarks):\n",
    "        #print(results.multi_hand_landmarks[0].landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP])\n",
    "\n",
    "    result[index] = [results.multi_hand_landmarks,mp_hands.HAND_CONNECTIONS]\n",
    "        \n",
    "    #Draw\n",
    "    \n",
    "    #Wait for race (Doesn't wait)\n",
    "    #if(flag_wait == 3):\n",
    "    #    flag_wait = 0\n",
    "    #flag_wait += 1\n",
    "\n",
    "# Classification Detection function    \n",
    "targets = {\n",
    "    1: \"call\",\n",
    "    2: \"dislike\",\n",
    "    3: \"fist\",\n",
    "    4: \"four\",\n",
    "    5: \"like\",\n",
    "    6: \"mute\",\n",
    "    7: \"ok\",\n",
    "    8: \"one\",\n",
    "    9: \"palm\",\n",
    "    10: \"peace\",\n",
    "    11: \"rock\",\n",
    "    12: \"stop\",\n",
    "    13: \"stop inverted\",\n",
    "    14: \"three\",\n",
    "    15: \"two up\",\n",
    "    16: \"two up inverted\",\n",
    "    17: \"three2\",\n",
    "    18: \"peace inverted\",\n",
    "    19: \"no gesture\"\n",
    "}\n",
    "COLOR = (0, 255, 0)\n",
    "FONT = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "class Demo:\n",
    "\n",
    "    @staticmethod\n",
    "    def preprocess(img: np.ndarray) -> Tuple[Tensor, Tuple[int, int], Tuple[int, int]]:\n",
    "        \"\"\"\n",
    "        Preproc image for model input\n",
    "        Parameters\n",
    "        ----------\n",
    "        img: np.ndarray\n",
    "            input image\n",
    "        \"\"\"\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        image = Image.fromarray(img)\n",
    "        width, height = image.size\n",
    "\n",
    "        image = ImageOps.pad(image, (max(width, height), max(width, height)))\n",
    "        padded_width, padded_height = image.size\n",
    "        image = image.resize((320, 320))\n",
    "\n",
    "        img_tensor = f.pil_to_tensor(image)\n",
    "        img_tensor = f.convert_image_dtype(img_tensor)\n",
    "        img_tensor = img_tensor[None, :, :, :]\n",
    "        return img_tensor, (width, height), (padded_width, padded_height)\n",
    "\n",
    "    @staticmethod\n",
    "    def run(detector: TorchVisionModel, num_hands: int = 2, threshold: float = 0.5, landmarks: bool = False, frame: str = None) -> None:\n",
    "        \"\"\"\n",
    "        if landmarks:\n",
    "            hands = mp.solutions.hands.Hands(\n",
    "                model_complexity=0,\n",
    "                static_image_mode=False,\n",
    "                max_num_hands=2,\n",
    "                min_detection_confidence=0.8)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        #frame = cv2.imread(image_path)    \n",
    "        #print(type(frame))\n",
    "        processed_frame, size, padded_size = Demo.preprocess(frame)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = detector(processed_frame)[0]\n",
    "        boxes = output[\"boxes\"][:num_hands]\n",
    "        scores = output[\"scores\"][:num_hands]\n",
    "        labels = output[\"labels\"][:num_hands]\n",
    "        \n",
    "        \n",
    "        # Processing the data   \n",
    "        result = [\"NO GESTURES\"]\n",
    "        count = 0\n",
    "        #print(scores)\n",
    "        for i in range(min(num_hands, len(boxes))):\n",
    "            if scores[i] > threshold:\n",
    "                count += 1\n",
    "                result = [targets[int(labels[i])], scores[i]]        \n",
    "        \n",
    "        #sprint(count)        \n",
    "        return result\n",
    "\n",
    "def _load_model(model_path: str, device: str) -> TorchVisionModel:\n",
    "    ssd_mobilenet = SSDMobilenet(num_classes=len(targets) + 1)\n",
    "    if not os.path.exists(model_path):\n",
    "        logging.info(f\"Model not found {model_path}\")\n",
    "        raise FileNotFoundError\n",
    "\n",
    "    ssd_mobilenet.load_state_dict(model_path, map_location=device)\n",
    "    ssd_mobilenet.eval()\n",
    "    return ssd_mobilenet\n",
    "\n",
    "model = _load_model(os.path.expanduser(\"SSDLite.pth\"),\"cpu\")\n",
    "\n",
    "\n",
    "def Classification(image,result,index):\n",
    "    # Classification Result\n",
    "    result= Demo.run(model, num_hands=100, threshold=0.75, landmarks=False,frame=image)\n",
    "    if(results[0] != \"NO GESTURES\"):\n",
    "        print(result[0])\n",
    "    \n",
    "    #Wait for race (Doesn't wait)\n",
    "    #if(flag_wait == 3):\n",
    "    #    flag_wait = 0\n",
    "    #flag_wait += 1\n",
    "    \n",
    "cap = cv2.VideoCapture(0)\n",
    "while cap.isOpened():\n",
    "    success, image = cap.read()\n",
    "    image.flags.writeable = True\n",
    "    \n",
    "    # Store Threads\n",
    "    start = time.time()\n",
    "    thread_list = [None] * 3\n",
    "    results = [None] * 3\n",
    "    \n",
    "    # Define Threads\n",
    "    thread_list[0] = Thread(target=Pose_detection, args=(results, 0,))\n",
    "    thread_list[1] = Thread(target=Hands_detection, args=(results, 1,))\n",
    "    thread_list[2] = Thread(target=Classification, args=(image,results,2,))\n",
    "    \n",
    "    # Start Threads\n",
    "    thread_list[0].start()\n",
    "    thread_list[1].start()\n",
    "    thread_list[2].start()\n",
    "    \n",
    "    # Join only Detectors\n",
    "    thread_list[0].join()\n",
    "    thread_list[1].join()\n",
    "    end1 = time.time()\n",
    "    #print(\"1. \"+str(end1-start))\n",
    "    \n",
    "    # Draw Image\n",
    "    start2 = time.time()\n",
    "    if results[0][0]:\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image,\n",
    "            results[0][0],\n",
    "            results[0][1],\n",
    "            landmark_drawing_spec=results[0][2])\n",
    "    \n",
    "    if results[1][0]:\n",
    "        for num, hand in enumerate(results[1][0]):\n",
    "            mp_drawing.draw_landmarks(image, hand, results[1][1], \n",
    "                                    mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=4),\n",
    "                                    mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2, circle_radius=4),\n",
    "                                     )\n",
    "    \n",
    "    #print(\"90909090909\")\n",
    "    \n",
    "    # Display\n",
    "    cv2.imshow('Hand Tracking', image)\n",
    "    end2 = time.time()\n",
    "    #print(\"2. \"+str(end2-start2))\n",
    "    \n",
    "    # Join classifier thread\n",
    "    thread_list[2].join()\n",
    "    \n",
    "    key = cv2.waitKey(5) & 0xFF\n",
    "    #print(key)\n",
    "    if (key == 27 or key == ord('q') or key == ord('x') or key == ord(\"c\")):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        sys.exit(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa9db35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('backend': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "b6e843b86af1659e1a9cea4da3ef71974b7f8012c0c1803bbdb0d57b488be4c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
